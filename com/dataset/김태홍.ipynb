{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#\ub370\uc774\ud130 \uc804\ucc98\ub9ac\n",
        "\n",
        "path = './com/dataset/'\n",
        "path_save= './com/dataset/'\n",
        "\n",
        "train_csv = pd.read_csv(path + 'train.csv', index_col=0)\n",
        "test_csv = pd.read_csv(path + 'test.csv', index_col=0)\n",
        "\n",
        "# print(train_csv) #[43865 rows x 35 columns]\n",
        "# print(test_csv) #[18798 rows x 35 columns]\n",
        "# print(train_csv.info()) #\uacb0\uce21\uce58 \uc5c6\uc74c\n",
        "\n",
        "###########################################################################################################\n",
        "# \uc2dc\uac01\ud654\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "font_path ='C:\\\\Windows\\\\Fonts\\\\gulim.ttc'\n",
        "fontprop = fm.FontProperties(fname=font_path, size=14)\n",
        "\n",
        "# X1~X30 \uc11c\ub85c\uc758 \uc0c1\uad00\uad00\uacc4\n",
        "selected_columns = train_csv.iloc[:, :30]\n",
        "correlation_matrix = selected_columns.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('x1\uc5d0\uc11c x30\ub07c\ub9ac\uc758 \uc0c1\uad00\uad00\uacc4',fontproperties=fontprop)\n",
        "plt.show()\n",
        "\n",
        "#\uc0c1\uad00\uad00\uacc4\uac00 0.99\ub118\ub294 \uc5f4\ub4e4 \ucd9c\ub825\n",
        "import numpy as np\n",
        "\n",
        "correlation_matrix = train_csv.iloc[:, :30].corr()\n",
        "\n",
        "high_corr = np.where((correlation_matrix.abs() >= 0.99) & (correlation_matrix.abs() < 1))\n",
        "\n",
        "print(\"0.99\uc774\uc0c1\uc778 \uc0c1\uad00\uad00\uacc4\")\n",
        "for i, j in zip(*high_corr):\n",
        "    if i != j and i < j:\n",
        "        print(f\"'{train_csv.columns[i]}','{train_csv.columns[j]}' ,{correlation_matrix.iloc[i, j]:.2f}\")\n",
        "\n",
        "# 'X12','X13' ,0.99\n",
        "# 'X12','X30' ,-0.99\n",
        "# 'X13','X22' ,1.00\n",
        "# 'X13','X25' ,1.00\n",
        "# 'X13','X30' ,-0.99\n",
        "# 'X22','X25' ,1.00\n",
        "\n",
        "\n",
        "# X1~X30\uc774\ub791 A, B, C, D, E \uce7c\ub7fc \uc0ac\uc774 \uc0c1\uad00\uad00\uacc4\n",
        "selected_columns = train_csv.iloc[:, :30].join(train_csv.iloc[:, -5:])\n",
        "correlation_matrix = selected_columns.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix.iloc[:30, -5:], annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('x1~30\uacfc ABCDE\uc758 \uc0c1\uad00\uad00\uacc4',fontproperties=fontprop)\n",
        "plt.show()\n",
        "\n",
        "# A, B, C, D, E \uce7c\ub7fc \uc11c\ub85c\uc758 \uc0c1\uad00\uad00\uacc4 \n",
        "selected_columns = train_csv[['A', 'B', 'C', 'D', 'E']]\n",
        "correlation_matrix = selected_columns.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('ABCDE\ub07c\ub9ac\uc758 \uc0c1\uad00\uad00\uacc4',fontproperties=fontprop)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#  \uac01 \uc5f4\uc758 \ubd84\ud3ec \uc2dc\uac01\ud654 \n",
        "for column in train_csv.columns:\n",
        "    sns.histplot(train_csv[column], kde=True) \n",
        "    plt.title(f'{column}\uc758 \ubd84\ud3ec',fontproperties=fontprop)\n",
        "    plt.xlabel('Values')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "    \n",
        "##################################################################################################################\n",
        "#0.99\ub118\ub294 \ub192\uc740 \uc0c1\uad00\uad00\uacc4 \uc5f4\ub07c\ub9ac feature engineering\n",
        "\n",
        "train_csv.insert(loc=0, column='col_6', value=train_csv.iloc[:, 21] + train_csv.iloc[:, 24])\n",
        "train_csv.insert(loc=0, column='col_5', value=train_csv.iloc[:, 12] + train_csv.iloc[:, 29])\n",
        "train_csv.insert(loc=0, column='col_4', value=train_csv.iloc[:, 12] + train_csv.iloc[:, 24])\n",
        "train_csv.insert(loc=0, column='col_3', value=train_csv.iloc[:, 12] + train_csv.iloc[:, 21])\n",
        "train_csv.insert(loc=0, column='col_2', value=train_csv.iloc[:, 11] + train_csv.iloc[:, 29])\n",
        "train_csv.insert(loc=0, column='col_1', value=train_csv.iloc[:, 11] + train_csv.iloc[:, 12])\n",
        "\n",
        "# print(train_csv)#[43865 rows x 41 columns]\n",
        "\n",
        "test_csv.insert(loc=0, column='col_6', value=test_csv.iloc[:, 21] + test_csv.iloc[:, 24])\n",
        "test_csv.insert(loc=0, column='col_5', value=test_csv.iloc[:, 12] + test_csv.iloc[:, 29])\n",
        "test_csv.insert(loc=0, column='col_4', value=test_csv.iloc[:, 12] + test_csv.iloc[:, 24])\n",
        "test_csv.insert(loc=0, column='col_3', value=test_csv.iloc[:, 12] + test_csv.iloc[:, 21])\n",
        "test_csv.insert(loc=0, column='col_2', value=test_csv.iloc[:, 11] + test_csv.iloc[:, 29])\n",
        "test_csv.insert(loc=0, column='col_1', value=test_csv.iloc[:, 11] + test_csv.iloc[:, 12])\n",
        "\n",
        "# print(test_csv)#[18798 rows x 41 columns]\n",
        "\n",
        "##################################################################################################################\n",
        "# \ub0ae\uc740 \uc0c1\uad00\uad00\uacc4 \uc81c\uac70\ud560 \uc5f4\n",
        "cols_to_drop = ['X1','X5', 'X10', 'X14']\n",
        "\n",
        "train_csv = train_csv.drop(cols_to_drop, axis=1)\n",
        "test_csv = test_csv.drop(cols_to_drop, axis=1)\n",
        "\n",
        "##################################################################################################################\n",
        "\n",
        "x = train_csv.iloc[:, :32]  \n",
        "y = train_csv.iloc[:, 32:] \n",
        "# print(x.shape, y.shape) \n",
        "# print(x)\n",
        "# print(y)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, train_size=0.8, random_state=1336, shuffle=True)\n",
        "\n",
        "#####################################################################################################################\n",
        "#\ubd84\ud3ec\uc5d0 \ub530\ub77c \ub530\ub85c\ub530\ub85c \uc2a4\ucf00\uc77c\ub9c1\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler, MaxAbsScaler, StandardScaler\n",
        "min_scaler = MinMaxScaler()\n",
        "rob_scaler = RobustScaler()\n",
        "max_scaler = MaxAbsScaler()\n",
        "sta_scaler = StandardScaler()\n",
        "\n",
        "scale_min_columns = ['X4','X6','X9','X11','X12','X13','X19','X20','X22','X25','X27','X30']\n",
        "\n",
        "x_train[scale_min_columns] \\\n",
        "    = min_scaler.fit_transform(x_train[scale_min_columns])\n",
        "\n",
        "x_test[scale_min_columns] \\\n",
        "    = min_scaler.transform(x_test[scale_min_columns])\n",
        "    \n",
        "test_csv[scale_min_columns] \\\n",
        "    = min_scaler.transform(test_csv[scale_min_columns])\n",
        "    \n",
        "####################################################\n",
        "\n",
        "scale_rob_columns = ['X2','X3','X7','X8','X15','X16','X17','X18','X21','X23','X24','X26','X28','X29']\n",
        "\n",
        "x_train[scale_rob_columns] \\\n",
        "    = rob_scaler.fit_transform(x_train[scale_rob_columns])\n",
        "\n",
        "x_test[scale_rob_columns] \\\n",
        "    = rob_scaler.transform(x_test[scale_rob_columns])\n",
        "    \n",
        "test_csv[scale_rob_columns] \\\n",
        "    = rob_scaler.transform(test_csv[scale_rob_columns])\n",
        "\n",
        "x_predict = test_csv.iloc[:, :32]  \n",
        "y_predict = test_csv.iloc[:, 32:]\n",
        "\n",
        "# print(x_predict)\n",
        "# print(y_predict)\n",
        "    \n",
        "##################################################################################################################################\n",
        "# #polynomial feature\n",
        "\n",
        "# from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# poly = PolynomialFeatures(degree=2) \n",
        "\n",
        "# x_train_poly = poly.fit_transform(x_train)\n",
        "# x_test_poly = poly.transform(x_test)\n",
        "# x_predict_poly = poly.transform(x_predict)\n",
        "\n",
        "# x_train = pd.concat([x_train.reset_index(drop=True), pd.DataFrame(x_train_poly[:, len(x_train.columns):])], axis=1)\n",
        "# x_test = pd.concat([x_test.reset_index(drop=True), pd.DataFrame(x_test_poly[:, len(x_test.columns):])], axis=1)\n",
        "# x_predict = pd.concat([x_predict.reset_index(drop=True), pd.DataFrame(x_predict_poly[:, len(x_predict.columns):])], axis=1)\n",
        "\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "# print(x_predict.shape)\n",
        "\n",
        "########################################################################################################################################\n",
        "#pca\n",
        "# from sklearn.decomposition import PCA\n",
        "\n",
        "# n_components = 15 \n",
        "\n",
        "# pca = PCA(n_components=n_components)\n",
        "\n",
        "# x_train = pca.fit_transform(x_train)\n",
        "# x_test = pca.transform(x_test)\n",
        "# x_predict = pca.transform(x_predict)\n",
        "\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "# print(x_predict.shape)\n",
        "\n",
        "##################################################################################################################################\n",
        "#\ubaa8\ub378\uad6c\uc131\n",
        "\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 300, 500],\n",
        "    'max_depth': [5, 10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "\n",
        "model_A = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=param_grid_rf, n_iter=100, cv=3, verbose=2, random_state=1336, n_jobs=-1)\n",
        "model_B = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=param_grid_rf, n_iter=100, cv=3, verbose=2, random_state=1336, n_jobs=-1)\n",
        "model_C = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=param_grid_rf, n_iter=100, cv=3, verbose=2, random_state=1336, n_jobs=-1)\n",
        "model_D = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=param_grid_rf, n_iter=100, cv=3, verbose=2, random_state=1336, n_jobs=-1)\n",
        "model_E = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=param_grid_rf, n_iter=100, cv=3, verbose=2, random_state=1336, n_jobs=-1)\n",
        "\n",
        "\n",
        "multi_output_model = MultiOutputRegressor(estimator=RandomForestRegressor())\n",
        "\n",
        "multi_output_model.estimators_ = [model_A, model_B, model_C, model_D, model_E]\n",
        "\n",
        "multi_output_model.fit(x_train, y_train)\n",
        "\n",
        "predictions = multi_output_model.predict(x_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print('mae:', mae)\n",
        "\n",
        "predict_test_csv = multi_output_model.predict(x_predict)\n",
        "mae_predict = mean_absolute_error(y_predict, predict_test_csv)\n",
        "print('mae_test_csv:', mae_predict)\n",
        "\n",
        "###############################################################################################\n",
        "from collections import Counter\n",
        "\n",
        "most_common_values = {\n",
        "    'A': Counter(train_csv['A']).most_common(1)[0][0],\n",
        "    'B': Counter(train_csv['B']).most_common(1)[0][0],\n",
        "    'C': Counter(train_csv['C']).most_common(1)[0][0],\n",
        "    'D': Counter(train_csv['D']).most_common(1)[0][0],\n",
        "    'E': Counter(train_csv['E']).most_common(1)[0][0]\n",
        "}\n",
        "\n",
        "predicted_values = multi_output_model.predict(x_predict)\n",
        "\n",
        "better_counter = 0  \n",
        "rows_with_3_better = [] \n",
        "\n",
        "for index, row in enumerate(predicted_values):\n",
        "    better_count = 0 \n",
        "    for i, value in enumerate(row):\n",
        "        if i < 3:\n",
        "            if value > most_common_values[chr(65 + i)]:\n",
        "                better_count += 1\n",
        "        else:\n",
        "            if value < most_common_values[chr(65 + i)]:\n",
        "                better_count += 1\n",
        "    \n",
        "    if better_count >= 3:\n",
        "        better_counter += 1\n",
        "        rows_with_3_better.append((index, row))\n",
        "\n",
        "\n",
        "if better_counter > 0:\n",
        "    print(f'better\uac00 3\uac1c \uc774\uc0c1\uc778 \ub370\uc774\ud130 \uac2f\uc218 : {better_counter}')\n",
        "    for idx, prediction in rows_with_3_better:\n",
        "        print(f\"\ud589 {idx}: \uc608\uce21\uac12: {prediction}\")\n",
        "else:\n",
        "    print('\uacb0\uacfc\uc5c6\uc74c')\n",
        "\n",
        "#####################################################################\n",
        "# \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud574\uc11c A,B,C,D,E \uac01 \uad00\ub9ac\ubcc0\uc218\ub97c \uc608\uce21\ud569\ub2c8\ub2e4\n",
        "# \uad00\ub9ac\ubcc0\uc218 A,B,C,D,E \uc758 \ub370\uc774\ud130\ub0b4\uc5d0\uc11c \uac00\uc7a5 \ube48\ub3c4\uc218\uac00 \ub192\uc740 \uac12\uacfc \uc608\uce21\uac12\uc744 \ube44\uad50\ud558\ub3c4\ub85d \ud558\uc600\uc2b5\ub2c8\ub2e4.\n",
        "\n",
        "# A,B,C\ub294 \uc608\uce21\uac12\uc774 \ucd5c\ube48\uac12 \uc774\uc0c1\uc774\uba74 better \n",
        "# D,E\ub294 \uc608\uce21\uac12\uc774 \ucd5c\ube48\uac12 \uc774\ud558\uc774\uba74 better\ub85c \ub098\ud0c0\ub0b4\ub3c4\ub85d \ud558\uc600\uc2b5\ub2c8\ub2e4.\n",
        "\n",
        "# A,B,C,D,E\ub97c \uc608\uce21\ud55c 5\uac00\uc9c0 \uac12\uc911 3\uac00\uc9c0 \uc774\uc0c1\uc774 better\ub85c \ub098\ud0c0\ub098\uba74 \uadf8 \uc870\ud569\ub4e4\uc774 \uba87\ud589\uc778\uc9c0\uc640 \uc608\uce21\ud55c A,B,C,D,E\uac12\uc744 \ucd9c\ub825\ud558\ub3c4\ub85d \ubaa8\ub378\ub9c1 \ud558\uc600\uc2b5\ub2c8\ub2e4.\n",
        "\n",
        "# \uacf5\uc815\uacfc\uc815\uc5d0\uc11c \uc870\uc5c5\ubcc0\uc218\ub97c \uc5b4\ub5bb\uac8c \uc124\uc815\ud560\uc9c0 \ub370\uc774\ud130\ub97c \ubbf8\ub9ac \ub123\uace0 A,B,C,D,E \uad00\ub9ac\ubcc0\uc218 \uc608\uce21\uc744 \ud1b5\ud574 better\uac00 3\uac00\uc9c0 \uc774\uc0c1\uc778 \uc870\uc5c5\ubcc0\uc218 \uc870\ud569\uc744 \uc0ac\uc6a9\ud55c\ub2e4\uba74 \uc774\uc775\uc774 \ub420\uac70\ub77c \uc0dd\uac01\ud569\ub2c8\ub2e4.\n",
        "\n",
        "# \ub354 \ud070 \uc774\ub4dd\uc744 \ubcf4\uace0\uc790 \ud55c\ub2e4\uba74 \ucd5c\ube48\uac12\uc774 \uc544\ub2cc \uac01 \uad00\ub9ac\ubcc0\uc218 \ub370\uc774\ud130 \uc218\uce58\uc758 0.75\uc9c0\uc810\uc5d0 \uc704\uce58\ud55c \uac12\ubcf4\ub2e4 \ud070 \uac12\uc744 better\ub85c \ub098\ud0c0\ub0b4\ub3c4\ub85d \uc218\uc815\ud560\uc218 \uc788\uace0, \uadf8 \uc218\uce58\ub610\ud55c \ubcc0\uacbd\ud560\uc218 \uc788\uc2b5\ub2c8\ub2e4\n",
        "# better\uc758 \uac2f\uc218 \ub610\ud55c 5\uac00\uc9c0\uc911 3\uac00\uc9c0 \uc774\uc0c1\uc774 \uc544\ub2cc 4\uac00\uc9c0 \ub610\ub294 5\uac00\uc9c0\ub85c \ubcc0\uacbd\ud560\uc218 \uc788\uc2b5\ub2c8\ub2e4."
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}